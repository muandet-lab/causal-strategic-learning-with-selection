{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tqdm.notebook as tqdm\n",
    "from multiprocessing import Pool\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import py.data_gen as data_gen\n",
    "import py.algos as algos\n",
    "\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "\n",
    "# IMPORTANT: please set MODE, N_CORES (if MODE=='compute') and exp_dir_load (if MODE=='load')\n",
    "MODE = 'compute'\n",
    "assert MODE in ('compute', 'load')\n",
    "N_CORES = 1 # if MODE=='compute', how many cores to use.\n",
    "exp_dir_load = \"experiments/3848a72\" # if MODE=='load', then set the directory to load results from.\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 18})\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "\n",
    "def save_runs(exp_root, exp_name, runs, args):\n",
    "    git_hash = algos.get_git_revision_hash()\n",
    "    dirty = b\"script.py\" in subprocess.check_output([\"git\", \"diff\", \"--name-only\"])\n",
    "    exp_root = os.path.join(exp_root, f\"{git_hash}-dirty\" if dirty else f\"{git_hash}\")\n",
    "    exp_dir = os.path.join(exp_root, exp_name)\n",
    "    os.makedirs(exp_dir)\n",
    "    with open(os.path.join(exp_dir, \"results\"), \"wb\") as f:\n",
    "        pickle.dump(runs, f)\n",
    "    with open(os.path.join(exp_dir, \"args\"), \"w\") as f:\n",
    "        json.dump(vars(args), f, indent=True, sort_keys=True, cls=NumpyEncoder)\n",
    "\n",
    "\n",
    "def get_accepted_per_round(_runs, n_rounds):\n",
    "    _list1 = []\n",
    "    for _r in _runs:\n",
    "        accepted_per_round = np.array(np.array_split(_r, n_rounds)).sum(axis=-1)\n",
    "        rejected_per_round = 1 - accepted_per_round\n",
    "        _list1.append(accepted_per_round)\n",
    "    accepted_per_round = np.concatenate(_list1)\n",
    "    return accepted_per_round"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## harris et. al setup with selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data.\n",
    "cmd = f\"--num-applicants 100000 --applicants-per-round 1000 --clip --envs-accept-rates .25 --rank-type uniform\"\n",
    "args = algos.get_args(cmd)\n",
    "pref_vect = args.pref_vect\n",
    "np.random.seed(1)\n",
    "(\n",
    "    b,\n",
    "    x,\n",
    "    y,\n",
    "    EW,\n",
    "    theta,\n",
    "    w,\n",
    "    z,\n",
    "    y_hat,\n",
    "    adv_idx,\n",
    "    disadv_idx,\n",
    "    g,\n",
    "    theta_star,\n",
    ") = data_gen.generate_data(\n",
    "    args.num_applicants, args.applicants_per_round, args.fixed_effort_conversion, args\n",
    ")\n",
    "\n",
    "cmd = f\"--num-applicants 100000 --applicants-per-round 1000 --clip --envs-accept-rates .25 --rank-type prediction\"\n",
    "args = algos.get_args(cmd)\n",
    "pref_vect = args.pref_vect\n",
    "np.random.seed(1)\n",
    "(\n",
    "    b,\n",
    "    x,\n",
    "    y2,\n",
    "    EW,\n",
    "    theta,\n",
    "    w,\n",
    "    z2,\n",
    "    y_hat,\n",
    "    adv_idx,\n",
    "    disadv_idx,\n",
    "    g,\n",
    "    theta_star,\n",
    ") = data_gen.generate_data(\n",
    "    args.num_applicants, args.applicants_per_round, args.fixed_effort_conversion, args\n",
    ")\n",
    "fig, ax = plt.subplots()\n",
    "plt.hist(y2[0], alpha=0.2, color=\"k\", bins=50)\n",
    "plt.hist(y2[0][z2 == 1], color=\"g\", bins=50, alpha=0.3, label=\"sampling by rank\")\n",
    "plt.hist(y[0][z == 1], color=\"b\", bins=50, alpha=0.3, label=\"sampling uniformly\")\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"College GPA ($Y_t$)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.legend()\n",
    "\n",
    "# assert np.all (y2[0] ==y[0] )\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/haris-et-al-selection-dist.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment: 2SLS under selection bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE=='compute':\n",
    "    n_runs = 10 \n",
    "    # ranking selection\n",
    "    cmd = f\"--num-applicants 100000 --applicants-per-round 1000 --clip --methods ols 2sls --envs-accept-rate .25 --rank-type prediction\"\n",
    "    args = algos.get_args(cmd)\n",
    "    args_list = [(s, args) for s in np.arange(n_runs)]\n",
    "    with Pool(N_CORES) as p:\n",
    "        runs = p.starmap(algos.run_multi_env, args_list)\n",
    "    runs = [r[0] for r in runs]\n",
    "\n",
    "    # uniform sampling\n",
    "    cmd = f\"--num-applicants 100000 --applicants-per-round 1000 --clip --methods ols 2sls --envs-accept-rate .25 --rank-type uniform\"\n",
    "    args = algos.get_args(cmd)\n",
    "    args_list = [(s, args) for s in np.arange(n_runs)]\n",
    "    with Pool(N_CORES) as p:\n",
    "        runs2 = p.starmap(algos.run_multi_env, args_list)\n",
    "    runs2 = [r[0] for r in runs2]\n",
    "\n",
    "    # saving stuff.\n",
    "    try:\n",
    "        save_runs(\"experiments\", \"harris-with-selection\", (runs, runs2), args)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    # loading saved stuff\n",
    "    with open(os.path.join(exp_dir_load, \"harris-with-selection\", \"results\"), \"rb\") as f:\n",
    "        runs = pickle.load(f)\n",
    "        runs, runs2 = runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = algos.runs2df(runs)\n",
    "df[\"sampling\"] = \"sampling by rank\"\n",
    "df2 = algos.runs2df(runs2)\n",
    "df2[\"sampling\"] = \"sampling uniformly\"\n",
    "df = pd.concat((df, df2))\n",
    "# converting to long format.\n",
    "# df = script.runs2df(runs)\n",
    "dflong = pd.melt(\n",
    "    df,\n",
    "    id_vars=(\"iterations\", \"sampling\"),\n",
    "    value_vars=(\"ols_env0\", \"2sls_env0\"),\n",
    "    var_name=\"method\",\n",
    "    value_name=\"error\",\n",
    ")\n",
    "dflong[\"method\"] = dflong[\"method\"].astype(\"category\")\n",
    "dflong[\"method\"] = dflong[\"method\"].cat.rename_categories(\n",
    "    {\"ols_env0\": \"OLS\", \"2sls_env0\": \"2SLS\"}\n",
    ")\n",
    "\n",
    "dflong[\"iterations\"] = dflong[\"iterations\"] + 2\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(\n",
    "    data=dflong, x=\"iterations\", y=\"error\", hue=\"method\", style=\"sampling\", ax=ax\n",
    ")\n",
    "ax.set_ylim(bottom=-0.001, top=0.2)\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_ylabel(r\"$||\\theta^* - \\hat{\\theta}^*|| $\")\n",
    "ax.set_xlabel(\"Rounds (t)\")\n",
    "ax.grid()\n",
    "ax.set_xlim((0, 100))\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "handles = [handles[1], handles[2], handles[4], handles[5]]\n",
    "labels = [labels[1], labels[2], labels[4], labels[5]]\n",
    "ax.legend(\n",
    "    handles=handles,\n",
    "    labels=labels,\n",
    "    ncol=2,\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.25, 1, 0.5, 0.5),\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/harris-et-al-selection-results.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# our setup: data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f\"--num-applicants 100000 --applicants-per-round 1000 --scaled-duplicates sequence --fixed-effort-conversion --methods 2sls ols ours --envs-accept-rate .5 --clip --alpha 1\"\n",
    "\n",
    "args = algos.get_args(cmd)\n",
    "pref_vect = args.pref_vect\n",
    "np.random.seed(1)\n",
    "(\n",
    "    b,\n",
    "    x,\n",
    "    y,\n",
    "    EW,\n",
    "    theta,\n",
    "    w,\n",
    "    z,\n",
    "    y_hat,\n",
    "    adv_idx,\n",
    "    disadv_idx,\n",
    "    g,\n",
    "    theta_star,\n",
    ") = data_gen.generate_data(\n",
    "    args.num_applicants, args.applicants_per_round, args.fixed_effort_conversion, args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.shape, b.shape, y.shape \n",
    "\n",
    "# idx = 1\n",
    "# # max_cov = np.max(np.concat rx[:, idx], \n",
    "# max_cov = np.max (np.concatenate ((x[:, idx], b[:, idx])))\n",
    "# min_cov = np.min (np.concatenate ((x[:, idx], b[:, idx])))\n",
    "\n",
    "# min_cov, max_cov "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with disadvantaged students.\n",
    "df0 = pd.DataFrame(\n",
    "    {\n",
    "        r\"$b_t^{SAT}$\": b[disadv_idx, 0],\n",
    "        r\"$b_t^{HS GPA}$\": b[disadv_idx, 1],\n",
    "        r\"$x_t^{SAT}$\": x[disadv_idx, 0],\n",
    "        r\"$x_t^{HS GPA}$\": x[disadv_idx, 1],\n",
    "        \"status\": z[disadv_idx],\n",
    "        \"y\": y[0, disadv_idx],\n",
    "        \"y_hat\": y_hat[0, disadv_idx],\n",
    "    }\n",
    ")\n",
    "\n",
    "# dataframe with advantaged students.\n",
    "df1 = pd.DataFrame(\n",
    "    {\n",
    "        r\"$b_t^{SAT}$\": b[adv_idx, 0],\n",
    "        r\"$b_t^{HS GPA}$\": b[adv_idx, 1],\n",
    "        r\"$x_t^{SAT}$\": x[adv_idx, 0],\n",
    "        r\"$x_t^{HS GPA}$\": x[adv_idx, 1],\n",
    "        \"status\": z[adv_idx],\n",
    "        \"y\": y[0, adv_idx],\n",
    "        \"y_hat\": y_hat[0, adv_idx],\n",
    "    }\n",
    ")\n",
    "\n",
    "df0[\"class\"] = \"disadv\"\n",
    "df1[\"class\"] = \"adv\"\n",
    "df = pd.concat((df0, df1))\n",
    "df[\"status\"] = df.status.astype(\"category\").cat.rename_categories(\n",
    "    {0.0: \"rejected\", 1.0: \"accepted\"}\n",
    ")\n",
    "df[\"status\"]\n",
    "\n",
    "# SAT score\n",
    "dflong = pd.melt(\n",
    "    df,\n",
    "    id_vars=(\"class\"),\n",
    "    value_vars=(r\"$b_t^{SAT}$\", r\"$x_t^{SAT}$\"),\n",
    "    var_name=\"time\",\n",
    "    value_name=\"score\",\n",
    ")\n",
    "dflong\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, sharex=True, sharey=True)\n",
    "g = sns.histplot(dflong[dflong[\"class\"] == \"disadv\"], x=\"score\", hue=\"time\", ax=ax[0])\n",
    "g.legend_.set_title(None)\n",
    "g = sns.histplot(dflong[dflong[\"class\"] == \"adv\"], x=\"score\", hue=\"time\", ax=ax[1])\n",
    "g.legend_.set_title(None)\n",
    "ax[1].set_xlabel(\"SAT score\")\n",
    "\n",
    "\n",
    "# four mean lines\n",
    "ax[0].axvline(\n",
    "    x=dflong[\n",
    "        (dflong[\"class\"] == \"disadv\") & (dflong.time == r\"$b_t^{SAT}$\")\n",
    "    ].score.mean(),\n",
    "    color=sns.color_palette()[0],\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax[0].axvline(\n",
    "    x=dflong[\n",
    "        (dflong[\"class\"] == \"disadv\") & (dflong.time == r\"$x_t^{SAT}$\")\n",
    "    ].score.mean(),\n",
    "    color=sns.color_palette()[1],\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "ax[1].axvline(\n",
    "    x=dflong[(dflong[\"class\"] == \"adv\") & (dflong.time == r\"$b_t^{SAT}$\")].score.mean(),\n",
    "    color=sns.color_palette()[0],\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax[1].axvline(\n",
    "    x=dflong[(dflong[\"class\"] == \"adv\") & (dflong.time == r\"$x_t^{SAT}$\")].score.mean(),\n",
    "    color=sns.color_palette()[1],\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/our-settings-data1.png\")\n",
    "\n",
    "# GPA\n",
    "dflong = pd.melt(\n",
    "    df,\n",
    "    id_vars=(\"class\"),\n",
    "    value_vars=(r\"$b_t^{HS GPA}$\", r\"$x_t^{HS GPA}$\"),\n",
    "    var_name=\"time\",\n",
    "    value_name=\"score\",\n",
    ")\n",
    "fig, ax = plt.subplots(nrows=2, sharex=True, sharey=True)\n",
    "g = sns.histplot(dflong[dflong[\"class\"] == \"disadv\"], x=\"score\", hue=\"time\", ax=ax[0])\n",
    "g.legend_.set_title(None)\n",
    "g = sns.histplot(dflong[dflong[\"class\"] == \"adv\"], x=\"score\", hue=\"time\", ax=ax[1])\n",
    "g.legend_.set_title(None)\n",
    "ax[1].set_xlabel(\"High School GPA\")\n",
    "\n",
    "# four mean lines\n",
    "ax[0].axvline(\n",
    "    x=dflong[\n",
    "        (dflong[\"class\"] == \"disadv\") & (dflong.time == r\"$b_t^{HS GPA}$\")\n",
    "    ].score.mean(),\n",
    "    color=sns.color_palette()[0],\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax[0].axvline(\n",
    "    x=dflong[\n",
    "        (dflong[\"class\"] == \"disadv\") & (dflong.time == r\"$x_t^{HS GPA}$\")\n",
    "    ].score.mean(),\n",
    "    color=sns.color_palette()[1],\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "ax[1].axvline(\n",
    "    x=dflong[\n",
    "        (dflong[\"class\"] == \"adv\") & (dflong.time == r\"$b_t^{HS GPA}$\")\n",
    "    ].score.mean(),\n",
    "    color=sns.color_palette()[0],\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax[1].axvline(\n",
    "    x=dflong[\n",
    "        (dflong[\"class\"] == \"adv\") & (dflong.time == r\"$x_t^{HS GPA}$\")\n",
    "    ].score.mean(),\n",
    "    color=sns.color_palette()[1],\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/our-settings-data2.png\")\n",
    "\n",
    "# dfa, dfr = pd.DataFrame({'y': y[0][z==1]}), pd.DataFrame({'y': y[0][z==0]})\n",
    "# df = pd.concat((dfa, dfr))\n",
    "fig, ax = plt.subplots()\n",
    "g = sns.histplot(df, x=\"y\", hue=\"status\", ax=ax, multiple=\"stack\")\n",
    "g.legend_.set_title(None)\n",
    "ax.set_xlabel(\"College GPA\")\n",
    "ax.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/our-settings-data3.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment: single environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE=='compute':\n",
    "    n_runs = 10 \n",
    "    args = algos.get_args(cmd)\n",
    "    args_list = [(s, args) for s in np.arange(n_runs)]\n",
    "    with Pool(N_CORES) as p:\n",
    "        runs = p.starmap(algos.run_multi_env, args_list)\n",
    "        # _runs = [r[2] for r in runs]\n",
    "    runs = [r[0] for r in runs]\n",
    "\n",
    "    # saving stuff.\n",
    "    try:\n",
    "        save_runs(\"experiments\", \"our-settings-single-env\", runs, args)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    # loading saved stuff\n",
    "    with open(os.path.join(exp_dir_load, \"our-settings-single-env\", \"results\"), \"rb\") as f:\n",
    "        runs = pickle.load(f)\n",
    "    runs\n",
    "    df2 = algos.runs2df(runs)\n",
    "    df = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = algos.runs2df(runs)\n",
    "# long format for plotting\n",
    "value_vars = [f\"{m}_env{e}\" for m in args.methods for e in range(args.num_envs)]\n",
    "dflong = pd.melt(\n",
    "    df, id_vars=\"iterations\", value_vars=value_vars, var_name=\"env\", value_name=\"error\"\n",
    ")\n",
    "dflong\n",
    "dflong[\"method\"] = dflong.env.apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "dflong[\"env\"] = dflong.env.apply(lambda x: x.split(\"_\")[-1])\n",
    "dflong[\"method\"] = dflong[\"method\"].astype(\"category\")\n",
    "dflong[\"method\"] = dflong[\"method\"].cat.rename_categories(\n",
    "    {\"ours\": \"MSLR\", \"ols\": \"OLS\", \"2sls\": \"2SLS\"}\n",
    ")\n",
    "dflong\n",
    "dflong_stack = dflong  # remember for later.\n",
    "dflong_stack[\"iterations\"] = dflong_stack[\"iterations\"] + 2\n",
    "dflong_stack\n",
    "plt.rcParams.update({\"font.size\": 18})\n",
    "fig, ax = plt.subplots(figsize=(7, 3.5))\n",
    "sns.lineplot(\n",
    "    dflong_stack,\n",
    "    x=\"iterations\",\n",
    "    y=\"error\",\n",
    "    errorbar=(\"ci\", 95),\n",
    "    ax=ax,\n",
    "    hue=\"method\",\n",
    "    markers={\"MSLR\": \"^\", \"OLS\": \"X\", \"2SLS\": \"o\"},\n",
    "    markevery=10,\n",
    "    markersize=9,\n",
    ")\n",
    "ax.grid()\n",
    "ax.set_ylim(bottom=-0.001, top=0.3)\n",
    "ax.set_ylabel(r\"$|| \\theta^* - \\hat{\\theta}^* || $\")\n",
    "ax.set_xlabel(\"Rounds (t)\")\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(\n",
    "    handles=handles[:],\n",
    "    labels=labels[:],\n",
    "    ncol=3,\n",
    "    loc=\"lower left\",\n",
    "    bbox_to_anchor=(0, 1, 0.5, 0.5),\n",
    ")\n",
    "ax.set_xlim((0, 100))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/our-settings-single-env.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment: sensitivity analysis (linearity assumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE=='compute':\n",
    "    dfs = []\n",
    "    for alpha in (0, .25, .5, .75, 1):\n",
    "        n_runs = 10 \n",
    "        cmd = f\"--num-applicants 100000 --applicants-per-round 1000 --scaled-duplicates sequence --fixed-effort-conversion --methods 2sls ols ours --envs-accept-rate .5 --clip --alpha {alpha}\"\n",
    "        args = algos.get_args(cmd)\n",
    "        args_list = [(s, args) for s in np.arange(n_runs)]\n",
    "        with Pool(N_CORES) as p:\n",
    "            runs = p.starmap(algos.run_multi_env, args_list)\n",
    "            # _runs = [r[2] for r in runs]\n",
    "        runs = [r[0] for r in runs]\n",
    "\n",
    "        df = algos.runs2df(runs)\n",
    "        df['alpha'] = alpha\n",
    "        dfs.append(df)\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    df = df[df['iterations']==df.iterations.max()] # filtering out the result of final iteration\n",
    "\n",
    "    # saving stuff.\n",
    "    try:\n",
    "        save_runs(\"experiments\", \"linearity-sens-single-env-causal\", dfs, args)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "elif MODE=='load':\n",
    "    with open('experiments/76b75cf/linearity-sens-single-env-causal/results', 'rb') as f:\n",
    "        dfs = pickle.load(f)\n",
    "    df = pd.concat(dfs)\n",
    "    df = df[df['iterations']==df.iterations.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long format, for plotting.\n",
    "value_vars = [f\"{m}_env{e}\" for m in args.methods for e in range(args.num_envs)]\n",
    "dflong = pd.melt(\n",
    "    df, id_vars=(\"alpha\",), value_vars=value_vars, var_name=\"env\", value_name=\"error\"\n",
    ")\n",
    "dflong\n",
    "dflong[\"method\"] = dflong.env.apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "dflong[\"env\"] = dflong.env.apply(lambda x: x.split(\"_\")[-1])\n",
    "\n",
    "dflong[\"method\"] = dflong[\"method\"].astype(\"category\")\n",
    "dflong[\"method\"] = dflong[\"method\"].cat.rename_categories(\n",
    "    {\"ours\": \"MSLR\", \"ols\": \"OLS\", \"2sls\": \"2SLS\"}\n",
    ")\n",
    "dflong \n",
    "\n",
    "sns.pointplot(\n",
    "    data=dflong, x='alpha', y='error', hue='method'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment: robustness on acceptance rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE=='compute':\n",
    "    ## selection function variation.\n",
    "    n_envs = 1\n",
    "    dfs = []\n",
    "    df_w = []\n",
    "    df_z = []\n",
    "    for env0_accept_rate in (0.2, 0.4, 0.6, 0.8, 1.0):\n",
    "        cmd = f\"--num-applicants 500000 --applicants-per-round 1000 --scaled-duplicates sequence --fixed-effort-conversion --normalize --methods 2sls ols ours --offline-eval --envs-accept-rates {env0_accept_rate}\"\n",
    "        args = algos.get_args(cmd)\n",
    "\n",
    "        n_runs = 10\n",
    "        args_list = [(s, args) for s in np.arange(n_runs)]\n",
    "        with Pool(N_CORES) as p:\n",
    "            runs_data = p.starmap(algos.run_multi_env, args_list)\n",
    "\n",
    "        # recording errors\n",
    "        runs = [r[0] for r in runs_data]\n",
    "        df = algos.runs2df(runs)\n",
    "        df[\"env0_accept_rate\"] = env0_accept_rate\n",
    "        dfs.append(df)\n",
    "\n",
    "        # recording z\n",
    "        runs_z = [r[2] for r in runs_data]\n",
    "        accepted_per_round = get_accepted_per_round(\n",
    "            runs_z, args.num_applicants / args.applicants_per_round\n",
    "        )\n",
    "        df = pd.DataFrame(\n",
    "            {\"accepted_per_round\": accepted_per_round, \"env0_accept_rate\": env0_accept_rate}\n",
    "        )\n",
    "        df[\"rejected_per_round\"] = args.applicants_per_round - df[\"accepted_per_round\"]\n",
    "        df_z.append(df)\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    # df_w = pd.concat(df_w)\n",
    "    df_z = pd.concat(df_z)\n",
    "    value_vars = [f\"{m}_env{ei}\" for m in args.methods for ei in range(args.num_envs)]\n",
    "    value_vars\n",
    "    dflong = pd.melt(\n",
    "        df,\n",
    "        id_vars=\"env0_accept_rate\",\n",
    "        value_vars=value_vars,\n",
    "        value_name=\"error\",\n",
    "        var_name=\"method_env\",\n",
    "    )\n",
    "    dflong[\"method\"] = dflong.method_env.apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "    dflong[\"env\"] = dflong.method_env.apply(lambda x: x.split(\"_\")[-1])\n",
    "\n",
    "    try:\n",
    "        save_runs(\"experiments\", \"our-settings-single-env-ablation\", dflong, args)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else: # load\n",
    "    with open(\n",
    "        os.path.join(exp_dir_load, \"our-settings-single-env-ablation\", \"results\"), \"rb\"\n",
    "    ) as f:\n",
    "        dflong = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "dflong[\"method\"] = dflong[\"method\"].astype(\"category\")\n",
    "dflong[\"method\"] = dflong[\"method\"].cat.rename_categories(\n",
    "    {\"2sls\": \"2SLS\", \"ols\": \"OLS\", \"ours\": \"MSLR\"}\n",
    ")\n",
    "sns.pointplot(\n",
    "    data=dflong[dflong.env == \"env0\"],\n",
    "    x=\"env0_accept_rate\",\n",
    "    y=\"error\",\n",
    "    hue=\"method\",\n",
    "    markers=\"^\",\n",
    ")\n",
    "ax.grid()\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(\n",
    "    handles=handles[:],\n",
    "    labels=labels[:],\n",
    "    ncol=3,\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, 1),\n",
    ")\n",
    "\n",
    "ax.set_xlabel(r\"$\\rho$\")\n",
    "ax.set_ylabel(r\"$|| \\theta^* - \\hat{\\theta}^*|| $\")\n",
    "plt.tight_layout()\n",
    "ax.invert_xaxis()\n",
    "plt.savefig(\"figures/our-setttings-single-env-ablation.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## our setup for multiple envs: data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f\"--num-applicants 100000 --applicants-per-round 1000 --scaled-duplicates sequence --fixed-effort-conversion --normalize --methods ours --num-envs 2 --theta-star-std 0.1 --envs-accept-rate .5 .5 --num-cooperative-envs 2\"\n",
    "args = algos.get_args(cmd)\n",
    "pref_vect = args.pref_vect \n",
    "np.random.seed(3)\n",
    "(\n",
    "    b,\n",
    "    x,\n",
    "    y,\n",
    "    EW,\n",
    "    theta,\n",
    "    w,\n",
    "    z,\n",
    "    y_hat,\n",
    "    adv_idx,\n",
    "    disadv_idx,\n",
    "    g,\n",
    "    theta_star,\n",
    ") = data_gen.generate_data(\n",
    "    args.num_applicants, args.applicants_per_round, args.fixed_effort_conversion, args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_envs(d, idx, var, prefix):\n",
    "    d.update(\n",
    "        {\n",
    "            f\"{prefix}_env{env_idx}\": var[env_idx, idx]\n",
    "            for env_idx in range(args.num_envs)\n",
    "        }\n",
    "    )\n",
    "    return d\n",
    "\n",
    "\n",
    "# disadvantage class\n",
    "df0 = {\n",
    "    \"b1\": b[disadv_idx, 0],\n",
    "    \"b2\": b[disadv_idx, 1],\n",
    "    \"status\": z[disadv_idx],\n",
    "    \"y\": y[0, disadv_idx],\n",
    "    \"y_hat\": y_hat[0, disadv_idx],\n",
    "    \"x1\": x[disadv_idx, 0],\n",
    "    \"x2\": x[disadv_idx, 1],\n",
    "}\n",
    "df0 = add_envs(df0, disadv_idx, y, \"y\")\n",
    "df0 = add_envs(df0, disadv_idx, w, \"w\")\n",
    "df0.keys()\n",
    "\n",
    "df0 = pd.DataFrame(df0)\n",
    "df0[\"class\"] = \"disadv\"\n",
    "\n",
    "# advantaged class\n",
    "df1 = {\n",
    "    \"b1\": b[adv_idx, 0],\n",
    "    \"b2\": b[adv_idx, 1],\n",
    "    \"status\": z[adv_idx],\n",
    "    \"y\": y[0, adv_idx],\n",
    "    \"y_hat\": y_hat[0, adv_idx],\n",
    "    \"x1\": x[adv_idx, 0],\n",
    "    \"x2\": x[adv_idx, 1],\n",
    "}\n",
    "df1 = add_envs(df1, adv_idx, y, \"y\")\n",
    "df1 = add_envs(df1, adv_idx, w, \"w\")\n",
    "df1.keys()\n",
    "\n",
    "df1 = pd.DataFrame(df1)\n",
    "df1[\"class\"] = \"adv\"\n",
    "df = pd.concat((df0, df1))\n",
    "df[\"status\"] = df[\"status\"].astype(\"category\")\n",
    "df\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# first plot\n",
    "df_z = pd.get_dummies(df[\"status\"]).to_numpy()\n",
    "df_z = np.array(\n",
    "    np.split(df_z, (args.num_applicants / args.applicants_per_round), axis=0)\n",
    ")\n",
    "ax.axhline(y=args.applicants_per_round, color=\"k\", linestyle=\"--\")\n",
    "\n",
    "assert df_z.shape == (\n",
    "    (args.num_applicants / args.applicants_per_round),\n",
    "    args.applicants_per_round,\n",
    "    args.num_envs + 1,\n",
    "), f\"{df_z.shape}\"\n",
    "df_z = df_z.sum(axis=1)\n",
    "sns.barplot(pd.DataFrame(df_z), ax=ax)\n",
    "ax.set_xlabel(r\"$z_t$\")\n",
    "\n",
    "# second plot\n",
    "fig, ax = plt.subplots()\n",
    "df = df.rename(columns={\"status\": r\"$Z_t$\"})\n",
    "# df[r'$Z_t$'] = df[r'$Z_t$'].cat.rename_categories({0.0: '0', 1.0: '1', 2.0: '2'})\n",
    "\n",
    "sns.histplot(df[df[r\"$Z_t$\"] != 0], x=\"x1\", hue=r\"$Z_t$\", ax=ax)\n",
    "ax.axvline(x=df[df[r\"$Z_t$\"] == 0].x1.mean(), color=\"b\", linestyle=\"--\")\n",
    "ax.axvline(x=df[df[r\"$Z_t$\"] == 1].x1.mean(), color=\"y\", linestyle=\"-.\")\n",
    "ax.axvline(x=df[df[r\"$Z_t$\"] == 2].x1.mean(), color=\"g\", linestyle=\"--\")\n",
    "ax.axvline(x=df[df[r\"$Z_t$\"] == 3].x1.mean(), color=\"r\", linestyle=\"--\")\n",
    "ax.set_xlabel(r\"$x_t^{(1)}$ (SAT score)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/our-settings-multi1.png\")\n",
    "\n",
    "# third plot\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(df[df[r\"$Z_t$\"] != 0], x=\"x2\", hue=r\"$Z_t$\", ax=ax)\n",
    "for env_idx, c in zip(range(args.num_envs + 1), (\"b\", \"y\", \"g\", \"r\")):\n",
    "    print(df[df[r\"$Z_t$\"] == env_idx].x2.mean())\n",
    "    ax.axvline(x=df[df[r\"$Z_t$\"] == env_idx].x2.mean(), c=c, linestyle=\"--\")\n",
    "\n",
    "ax.set_xlabel(r\"$x_t^{(2)}$ (HS GPA)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/our-settings-multi2.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment: protocol vs. no protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions. \n",
    "def plot_convergence(\n",
    "    df, y_col, y_range, y_label, hline_y, hline_label=None, pathname=None\n",
    "):\n",
    "    _, ax = plt.subplots(figsize=(7, 3.5))\n",
    "    sns.lineplot(\n",
    "        df[df.env == 1],\n",
    "        x=\"iterations\",\n",
    "        y=y_col,\n",
    "        hue=\"is-protocol\",\n",
    "        linestyle=\"dashdot\",\n",
    "        legend=False,\n",
    "    )\n",
    "    sns.lineplot(df[df.env == 0], x=\"iterations\", y=y_col, hue=\"is-protocol\")\n",
    "    ax.set_ylim(y_range)\n",
    "    ax.axhline(y=hline_y, color=\"k\", linestyle=\"--\", label=hline_label)\n",
    "\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_xlabel(r\"Rounds (t)\")\n",
    "    ax.grid()\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    lgd = ax.legend(\n",
    "        handles=handles[:],\n",
    "        labels=labels[:],\n",
    "        ncol=3,\n",
    "        loc=\"lower center\",\n",
    "        bbox_to_anchor=(0.5, 1),\n",
    "        handlelength=1,\n",
    "        labelspacing=0,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    ax.set_xlim(0, 300)\n",
    "\n",
    "    if pathname is not None:\n",
    "        plt.savefig(pathname)\n",
    "\n",
    "\n",
    "def get_data(runs, env_idx):\n",
    "    \"\"\"Gets a list of tuple. Each tupel is as returned by the function algos.run_multi_env.\n",
    "\n",
    "    Args:\n",
    "        runs (List):  A list of n_runs elements. Each element is a tuple as\n",
    "                               returned by the function algos.run_multi_env.\n",
    "        env_idx (integer): Index of envrionment for which to return the data.\n",
    "\n",
    "    Returns:\n",
    "        df: A dataframe with columns ['iterations', 'est_dim*', 'err_dim*', 'env']\n",
    "            having estimate for causal parameter per iteration.\n",
    "        Number of rows are (n_runs x n_iterations of the algorithm.)\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for r in runs:  # iterator\n",
    "        ests = np.array(r[1][f\"ours_env{env_idx}\"])\n",
    "        theta_star = r[-1][\"theta_star\"]\n",
    "        err = ests - theta_star[env_idx, :]\n",
    "        assert err.ndim == 2\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"env\": env_idx,\n",
    "                \"err_dim0\": err[:, 0],\n",
    "                \"err_dim1\": err[:, 1],\n",
    "                \"est_dim0\": ests[:, 0],\n",
    "                \"est_dim1\": ests[:, 1],\n",
    "            }\n",
    "        )\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs)\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename({\"index\": \"iterations\"}, axis=1, inplace=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE=='compute':\n",
    "    theta_star_std = 0.1\n",
    "    _runs = []\n",
    "    for nce in tqdm.tqdm((2, 1)):\n",
    "        cmd = f\"--num-applicants 300000 --applicants-per-round 1000 --scaled-duplicates sequence --fixed-effort-conversion --normalize --methods ours --num-envs 2 --theta-star-std {theta_star_std} --envs-accept-rates .5 --num-cooperative-envs {nce}\"\n",
    "\n",
    "        args = algos.get_args(cmd)\n",
    "        n_runs = 50\n",
    "        args_list = [(s, args) for s in np.arange(n_runs)]\n",
    "        with Pool(N_CORES) as p:\n",
    "            runs = p.starmap(algos.run_multi_env, args_list)\n",
    "        _runs.append(runs)\n",
    "\n",
    "    runs_protocol = _runs[0]\n",
    "    runs_no_protocol = _runs[1]\n",
    "\n",
    "    # some post processing before saving, to reduce the filesize.\n",
    "    runs_protocol = _runs[0]\n",
    "    runs_no_protocol = _runs[1]\n",
    "\n",
    "    dfs = []\n",
    "    for env_idx in (0, 1):\n",
    "        df = get_data(runs_protocol, env_idx)\n",
    "        df2 = get_data(runs_no_protocol, env_idx)\n",
    "        df[\"is-protocol\"] = \"Co-op\"\n",
    "        df2[\"is-protocol\"] = \"No co-op.\"\n",
    "        df = pd.concat((df, df2))\n",
    "        dfs.append(df)\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    df.reset_index(inplace=True)\n",
    "    df\n",
    "    try:\n",
    "        save_runs(\"experiments\", \"p-vs-np\", df, args)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    with open(os.path.join(exp_dir_load, \"p-vs-np\", \"results\"), \"rb\") as f:\n",
    "        df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(\n",
    "    df,\n",
    "    \"err_dim1\",\n",
    "    (-0.05, 0.05),\n",
    "    r\"$\\hat{\\theta}^{HS GPA}_{i} - \\theta^{*, HS GPA}_{i}$\",\n",
    "    0,\n",
    "    pathname=\"figures/protocol-vs-no-protocol4.pdf\",\n",
    ")\n",
    "plot_convergence(\n",
    "    df=df,\n",
    "    y_col=\"err_dim0\",\n",
    "    y_range=(-0.001, 0.001),\n",
    "    y_label=r\"$\\hat{\\theta}^{SAT}_{i} - \\theta^{*, SAT}_{i}$\",\n",
    "    hline_y=0,\n",
    "    pathname=\"figures/protocol-vs-no-protocol3.pdf\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment: protocol improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE=='compute':\n",
    "    theta_star_std = 0.1  # set to 0 for the same underlying causal parameters.\n",
    "    _runs = []\n",
    "    n_envs = 3\n",
    "    coop_envs = (3, 2)\n",
    "    for nce in tqdm.tqdm(coop_envs):\n",
    "        cmd = (\n",
    "            f\"--offline-eval --num-applicants 500000 --applicants-per-round 5000 --scaled-duplicates sequence \"\n",
    "            f\"--fixed-effort-conversion --normalize --methods ours --num-envs {n_envs} --theta-star-std {theta_star_std} \"\n",
    "            f\"--envs-accept-rates .5 --num-cooperative-envs {nce}\"\n",
    "        )\n",
    "        args = algos.get_args(cmd)\n",
    "        n_runs = 100\n",
    "        args_list = [(s, args) for s in np.arange(n_runs)]\n",
    "        with Pool(N_CORES) as p:\n",
    "            runs = p.starmap(algos.run_multi_env, args_list)\n",
    "            _runs.append(runs)\n",
    "    # some post-processing to save some disk space upon saving.\n",
    "    dfs = []\n",
    "    for i, nce in enumerate((coop_envs)):\n",
    "        df = algos.runs2df([r[0] for r in _runs[i]])\n",
    "        df[\"cooperative envs.\"] = (\n",
    "            r\"Full Cooperation\" if nce == 3 else r\"Partial Cooperation\"\n",
    "        )\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs)\n",
    "    df[\"ours_env0+ours_env1\"] = (df[\"ours_env0\"] + df[\"ours_env1\"]) / 2.0\n",
    "    df\n",
    "\n",
    "    try:\n",
    "        save_runs(\"experiments\", \"protocol-improvement\", df, args)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    with open(os.path.join(exp_dir_load, \"protocol-improvement\", \"results\"), \"rb\") as f:\n",
    "        df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_vars = (f\"ours_env{i}\" for i in range(3))\n",
    "dflong = pd.melt(\n",
    "    df,\n",
    "    id_vars=(\"iterations\", \"cooperative envs.\"),\n",
    "    value_vars=value_vars,\n",
    "    var_name=\"env\",\n",
    "    value_name=\"error\",\n",
    ")\n",
    "dflong\n",
    "\n",
    "rename_cats_args = {f\"ours_env{env_idx}\": f\"${env_idx+1}$\" for env_idx in range(3)}\n",
    "dflong[\"env\"] = dflong.env.astype(\"category\").cat.rename_categories(rename_cats_args)\n",
    "dflong\n",
    "fig, ax = plt.subplots(figsize=(7, 3.5))\n",
    "sns.barplot(\n",
    "    dflong,\n",
    "    x=\"env\",\n",
    "    hue=\"cooperative envs.\",\n",
    "    y=\"error\",\n",
    "    palette={\n",
    "        \"Full Cooperation\": sns.color_palette()[1],\n",
    "        \"Partial Cooperation\": sns.color_palette()[0],\n",
    "    },\n",
    "    hue_order=(\"Partial Cooperation\", \"Full Cooperation\"),\n",
    ")\n",
    "# ax.legend(bbox_to_anchor=(1,1), title='cooperative envs.')\n",
    "ax.grid()\n",
    "ax.set_ylabel(r\"$|| \\theta^* - \\hat{\\theta}^*||$\")\n",
    "ax.set_xlabel(\"Decision Maker\")\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(\n",
    "    handles=handles[:],\n",
    "    labels=labels[:],\n",
    "    ncol=2,\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, 1),\n",
    "    handlelength=1,\n",
    "    labelspacing=0,\n",
    ")\n",
    "ax.set_xticklabels((\"A\", \"B\", \"C\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/protocol-vs-no-protocol-group.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment: utility in one env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE=='compute':\n",
    "    cmd = (\n",
    "        f\"--num-applicants 100000 --applicants-per-round 1000 --scaled-duplicates sequence \"\n",
    "        \"--fixed-effort-conversion --normalize --methods ols ours --envs-accept-rate .5 --theta-star-std 0\"\n",
    "    )\n",
    "    cmd = f\"{cmd} --offline-eval\"\n",
    "\n",
    "    args = algos.get_args(cmd)\n",
    "    combos = [(\"theta_ao_hat\",), (\"theta_star_hat\",), (\"theta_ols_hat\",)]\n",
    "\n",
    "    dfs = []\n",
    "    for combo in combos:\n",
    "        n_runs = 100\n",
    "        args_list = [(args, t, combo) for t in range(n_runs)]\n",
    "        with Pool(N_CORES) as p:\n",
    "            results = p.starmap(algos.run_multi_env_utility, args_list)\n",
    "\n",
    "        results = [r[0] for r in results]  # filtering out the test theta array.\n",
    "        results = np.array(results)\n",
    "        pd_dict = {\n",
    "            f\"perf_env{env_idx}\": results[:, env_idx] for env_idx in range(args.num_envs)\n",
    "        }\n",
    "        pd_dict.update(\n",
    "            {f\"test_theta_env{env_idx}\": combo[env_idx] for env_idx in range(args.num_envs)}\n",
    "        )\n",
    "        df = pd.DataFrame(data=pd_dict)\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    try:\n",
    "        save_runs(\"experiments\", \"utility-single-env\", df, args)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    with open(os.path.join(exp_dir_load, \"utility-single-env\", \"results\"), \"rb\") as f:\n",
    "        df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# sns.pointplot(df, y=\"perf_env0\", x=\"test_theta_env0\", join=False)\n",
    "# ax.set_ylabel(r\"$\\mathcal{Q}(\\theta)$\")\n",
    "# ax.set_xticklabels(\n",
    "    # labels=[r\"$\\hat{\\theta}_{AO}$\", r\"$\\hat{\\theta}^*$\", r\"$\\hat{\\theta}_{OLS}$\"]\n",
    "# )\n",
    "# ax.grid()\n",
    "\n",
    "df_test = df.groupby(\"test_theta_env0\").agg((\"mean\", \"sem\"))\n",
    "df_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment: utility over multiple envs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE=='compute':\n",
    "    theta_star_std = 0.1\n",
    "    from itertools import product\n",
    "\n",
    "    options = (\"theta_ao_hat\", \"theta_ols_hat\", \"theta_star_hat\")\n",
    "    combos = [x for x in product(options, options)]\n",
    "\n",
    "    cmd = (\n",
    "        f\"--num-applicants 500000 --applicants-per-round 1000 --scaled-duplicates sequence \"\n",
    "        f\"--fixed-effort-conversion --normalize --methods ours ols --num-envs 2 --theta-star-std {theta_star_std} \"\n",
    "        f\"--envs-accept-rates .5 --num-cooperative-envs 2\"\n",
    "    )\n",
    "    cmd = f\"{cmd} --offline-eval\"\n",
    "    args = algos.get_args(cmd)\n",
    "    dfs = []\n",
    "    for combo in tqdm.tqdm(combos):\n",
    "        n_runs = 100\n",
    "        args_list = [(args, t, combo) for t in range(n_runs)]\n",
    "        with Pool(N_CORES) as p:\n",
    "            results = p.starmap(algos.run_multi_env_utility, args_list)\n",
    "\n",
    "        results = [r[0] for r in results]  # filtering out the test theta array.\n",
    "        results = np.array(results)\n",
    "        pd_dict = {\n",
    "            f\"perf_env{env_idx}\": results[:, env_idx] for env_idx in range(args.num_envs)\n",
    "        }\n",
    "        pd_dict.update(\n",
    "            {f\"test_theta_env{env_idx}\": combo[env_idx] for env_idx in range(args.num_envs)}\n",
    "        )\n",
    "        df = pd.DataFrame(data=pd_dict)\n",
    "        dfs.append(df)\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    try:\n",
    "        save_runs(\"experiments\", \"utility-multi-env\", df, args)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    with open(os.path.join(exp_dir_load, \"utility-multi-env\", \"results\"), \"rb\") as f:\n",
    "        df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"test_theta_env0\"] = df[\"test_theta_env0\"].astype(\"category\")\n",
    "df[\"test_theta_env1\"] = df[\"test_theta_env1\"].astype(\"category\")\n",
    "pd.melt(df, id_vars=\"perf_env0\", value_vars=(\"test_theta_env0\", \"test_theta_env1\"))\n",
    "df\n",
    "df_ = df[[\"perf_env0\", \"test_theta_env0\", \"test_theta_env1\"]]\n",
    "\n",
    "df_[\"test_theta_env1\"] = df_[\"test_theta_env1\"].cat.rename_categories(\n",
    "    {\n",
    "        \"theta_ao_hat\": r\"$\\hat{\\theta}^{AO}_2$\",\n",
    "        \"theta_star_hat\": r\"$\\hat{\\theta}^*_2$\",\n",
    "        \"theta_ols_hat\": r\"$\\hat{\\theta}_{OLS}$\",\n",
    "    }\n",
    ")\n",
    "df_ = df_.rename(columns={\"test_theta_env1\": r\"$\\theta_{2t}$\"})\n",
    "df_\n",
    "\n",
    "df_test = df_.groupby([r\"$\\theta_{2t}$\", \"test_theta_env0\"]).agg([\"mean\", \"sem\"])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "strategic-iv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
